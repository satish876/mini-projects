# -*- coding: utf-8 -*-
"""Multinomial Naive Bayes Classifier

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yLUcpnTrhHhdu3CGWBwP44Nx6iNLgLRk
"""

# from google.colab import drive
# drive.mount('/content/drive')

import numpy as np
import json
import re
import math
import time


categories = ['BUSINESS', 'COMEDY', 'SPORTS',
              'CRIME', 'RELIGION', 'HEALTHY LIVING', 'POLITICS']

# dataset splitting


def split_dataset(t):
    news_array = []
    for jobj in open('./news_category_dataset.json', 'r'):
        news = json.loads(jobj)

        # filter out unnecessary categories
        if news["category"] in categories:
            # filter out unnecessary attribues/features
            news_array.append({key: news[key]
                               for key in ["category", "headline"]})

    X_train = []  # training set
    y = []  # test set

    # creating a random boolean array, with True for t% data and False for rest
    # this will be used to split the dataset
    mask = np.random.rand(len(news_array)) < t
    # making the data at True indices as training data
    X_train = np.array(news_array)[mask]
    # rest of the 1-t% data will be considered as test data
    y = np.array(news_array)[~mask]

    # print(len(news_array), len(X_train), len(y))
    return X_train, y

preprocess_starttime = time.time()
X_train, y = split_dataset(0.85)

#  contains words of all headlines of each category, for training set
dataset = {category: [] for category in categories}
# vocab of training set
vocabulary = {}
category_relative_freq = {category: 0 for category in categories}
total_count = 0


# aggregating headlines of a category
# building up the vocabulary and couting word frequency
for news in X_train:
    category = news["category"]

    # total count of training data
    total_count += 1
    # count the relative freq of categories
    category_relative_freq[category] += 1

    # remove special characters and convert headline to lower case
    headline = re.sub('[^A-Za-z0-9]+', ' ', news["headline"].lower())
    # get the words in headline
    headline_words = headline.split()

    # adding headline to its category
    dataset[category] += headline_words

    # adding words to vocabulary and counting
    for word in headline_words:
        if word not in vocabulary:
            vocabulary[word] = 0
        else:
            vocabulary[word] = vocabulary[word] + 1

# filtering out words having atleast 2 occurances
# total_count = 0
filtered_vocab = {}
for word in vocabulary:
    if vocabulary[word] >= 2:
        # total_count += vocabulary[word]
        filtered_vocab[word] = vocabulary[word]

# counting word count for each category
word_count = {category: {} for category in categories}
for clas in dataset:
    category_data = dataset[clas]
    for word in category_data:
        if word not in word_count[clas]:
            word_count[clas][word] = 0
        else:
            word_count[clas][word] = word_count[clas][word] + 1


# for each word in a category, calculate ADD ONE SMOOTHENED UNIGRAM PROBABILITIES
probability = {category: {} for category in categories}
word_count_adjusted = {category: {} for category in categories}

for category in word_count:
    # this is for the words not in vocabulary
    probability[category]["NULL_undefined"] = 1 / \
        (len(dataset[category]) + len(filtered_vocab))

    # for words in the vocabulary
    for word in word_count[category]:
        probability[category][word] = (
            word_count[category][word] + 1) / (len(dataset[category]) + len(filtered_vocab))

        # adjust the count
        # word_count_adjusted[category][word] = ((word_count[category][word] + 1) * len(dataset[category])) / (len(dataset[category]) + len(vocabulary))

preprocess_endtime = time.time()

def C_NB(data):
    # remove special characters and convert input to lower case
    test_data = re.sub('[^A-Za-z0-9]+', ' ', data.lower())
    test_data_words = test_data.split()

    # will hold probabilities for each category
    probabilities = [0]*len(categories)
    for index in range(len(categories)):
        category = categories[index]

        # relative frequency of the category
        probabilities[index] += math.log(
            category_relative_freq[category] / total_count)

        for word in test_data_words:
            if word in dataset[category]:
                probabilities[index] += math.log(probability[category][word])
            else:
                probabilities[index] += math.log(
                    probability[category]["NULL_undefined"])

    # print(probabilities)
    # finding the argmax and returning
    argmax = max(enumerate(probabilities), key=lambda x: x[1])
    return categories[argmax[0]]

# run the model for the whole test data

overall_accuracy = []
category_accuracy = {category: [] for category in categories}

print("Time taken for preprocess and train model: %0.2fs\n" %(preprocess_endtime - preprocess_starttime))

for data in y:
    guess = C_NB(data["headline"])
    overall_accuracy.append(data["category"] == guess)
    category_accuracy[data["category"]].append(data["category"] == guess)


print("{:<20}{:<40}".format("Category", "Accuracy"))
print("*"*50)
for category in category_accuracy:
    if len(category_accuracy[category]) == 0:
        print("{:<20}{:<40}".format(category, "-"))
    else:
        print("{:<20}{:<40}".format(category, sum(category_accuracy[category])/len(category_accuracy[category])))

print("\n{:<20}{:<40}".format("Overall", sum(overall_accuracy)/len(overall_accuracy)))