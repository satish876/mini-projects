# -*- coding: utf-8 -*-
"""Multivariate Naive Bayes Classifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JYPaFQXk3Czdlg9HnrODockGCmOqWQeu
"""

import numpy as np
import json
import re
import math
import time

categories = ['BUSINESS', 'COMEDY', 'SPORTS',
              'CRIME', 'RELIGION', 'HEALTHY LIVING', 'POLITICS']


def split_dataset(t):
    news_array = []
    for jobj in open('news_category_dataset.json', 'r'):
        news = json.loads(jobj)

        # filter out unnecessary categories
        if news["category"] in categories:
            # filter out unnecessary attribues/features
            news_array.append({key: news[key]
                               for key in ["category", "headline"]})

    X_train = []  # training set
    y = []  # test set

    # creating a random boolean array, with True for t% data and False for rest
    # this will be used to split the dataset
    mask = np.random.rand(len(news_array)) < t
    # making the data at True indices as training data
    X_train = np.array(news_array)[mask]
    # rest of the 1-t% data will be considered as test data
    y = np.array(news_array)[~mask]

    # print(len(news_array), len(X_train), len(y))
    return X_train, y

def printTable(testCorrect,testCategories,acc):
    print("{:<20}{:<40}".format("Category", "Accuracy"))
    print("*"*50)
    for c in range(len(categories)):
        print("{:<20}{:<40}".format(categories[c], testCorrect[categories[c]]/(testCategories[categories[c]])))
    print("\n{:<20}{:<40}".format("Overall", acc))

def multivariate():
    #pre-computation getting respective headlines
    dataset={category: [] for category in categories}#stpring each headline as list of respective category
    vocabulary = {}
    total_news=0
    for news in X_train:#making dataset for categories and respective headlines
        total_news+=1
        category = news["category"]
        headline = re.findall(r'\w+', news["headline"]) 
        #print(headline)
        val = dataset.get(category)
        val.append(headline)
        dataset.update({category:val})
        for word in headline:#making vocabulary count
                if word not in vocabulary:
                    vocabulary[word] = 0
                else:
                    vocabulary[word] = vocabulary[word] + 1

    #print('dataset ',(dataset.get("BUSINESS")))
    filtered_vocab = {}
    filtered_vocab1=[]
    for word in vocabulary:
        if vocabulary[word] >= 2:
            # total_count += vocabulary[word]
            filtered_vocab1.append(word)
    preprocess_endtime = time.time()
    print('pre-process time taken-  %0.2fs'%(preprocess_endtime-preprocess_starttime))

    #performing training
    #count prob of each class
    probOfCat=[]#list will contain prob of a category/class as num of class documents/total number of documents respectively of newsCategory
    countOfCat=[]#num of docs who are cat
    training_starttime = time.time()
    for c in dataset:
        count = dataset[c]#get docs corresponding to category c
        probOfCat.append(len(count)/total_news)
    #print(probOfCat)


    vocLen = len(filtered_vocab1)#length of vocab
    condProb = np.zeros([len(categories),vocLen])#cond prob array
    catNum=0
    for c in dataset:
        headlines = dataset[c] #all headlines corresponding to given dataset
        count = len(headlines)
        vNumber=0
        for v in filtered_vocab1:#getting each vocabulary one by one 
            #find number of headlines having that word
            nct=0
            for headline in headlines:
                if v in headline:
                    nct+=1
            prob = (nct+1)/(count+vocLen)
            condProb[catNum][vNumber]=prob
            vNumber+=1
        catNum+=1
    #print(condProb)
    training_endtime = time.time()
    #print('time taken for training-  %0.2fs',training_endtime-training_starttime)
    print('time taken for training-  %0.2fs'%(training_endtime-training_starttime))

    ##test
    testCategories={category: 0 for category in categories}#use to store counts of headlines of each category in data and then getting accuracy for each category
    testCorrect={category: 0 for category in categories}
    res=0
    for news in y:
        scores=[] #for a news get all scores
        category = news["category"]
        testCategories[category]+=1
        headline = re.findall(r'\w+', news["headline"]) 
        for c in range(len(categories)):
            score=0
            score+=math.log(probOfCat[c])
            vNumber=0
            for v in filtered_vocab1:#getting each term in vocabulary
                if v in headline:
                    score+=math.log(condProb[c][vNumber])
                else:
                    score+=math.log(1-condProb[c][vNumber])
                vNumber+=1
            scores.append(score)
        argmax = scores.index(max(scores))  
        #print(category,categories[argmax])
        if category == categories[argmax]:
          testCorrect[category]+=1
          res+=1
        

    #print('Accuracy of Multivariate Bernoulli Naive Bayes Classifier-',res/len(y))
    printTable(testCorrect,testCategories,res/len(y))
    #for c in range(len(categories)):
        #print('Accuracy of ',categories[c],'- ',testCorrect[categories[c]]/testCategories[categories[c]])

preprocess_starttime = time.time()
X_train, y = split_dataset(0.85)
multivariate()

